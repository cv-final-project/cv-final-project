{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "GPUnose_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wHtJWO6jUDa"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # Displays a progress bar\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from pprint import pprint\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
        "from torch.utils.data.sampler import WeightedRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty-vqrISmTYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c59ca0-9f4c-4554-9241-1af0c99afeda"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU. You are good to go!\")\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    raise Exception(\"WARNING: Could not find GPU! Using CPU only. \\\n",
        "To enable GPU, please to go Edit > Notebook Settings > Hardware \\\n",
        "Accelerator and select GPU.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the GPU. You are good to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQphnO1XqS5F",
        "outputId": "d16f2540-76e2-4459-84c3-fe0f8e24341f"
      },
      "source": [
        "! rm -rf sample_data/\n",
        "! git clone https://eecs442finalproject:eecs442isgreat@github.com/cv-final-project/cv-final-project.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cv-final-project'...\n",
            "remote: Enumerating objects: 99236, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEnMl58lY9ew",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "2b3572f8-862f-4db6-e622-592fecb6d7ee"
      },
      "source": [
        "def get_min_num_files(class_d, tt):\n",
        "  mins = []\n",
        "  for class_, dirs in class_d.items():\n",
        "    t = sum([len(os.listdir(f'cv-final-project/data/small/{x}/{tt}')) for x in dirs])\n",
        "    mins.append(t)\n",
        "  return min(mins)\n",
        "def create_classes(class_d):\n",
        "  new_dir = f'org_data'\n",
        "  os.mkdir(f'{new_dir}/')\n",
        "  # print(min)\n",
        "  for tt in ['train', 'test']:\n",
        "    os.mkdir(f'{new_dir}/{tt}')\n",
        "    min = get_min_num_files(class_d, tt)\n",
        "    #################################################\n",
        "    if tt == 'train':\n",
        "      min = 1500\n",
        "    # else:\n",
        "    #   min = 5000\n",
        "    #################################################\n",
        "    for class_, dirs_ in class_d.items():\n",
        "      # print(class_, tt)\n",
        "      dirs = [f'cv-final-project/data/small/{x}/{tt}' for x in dirs_]\n",
        "      os.mkdir(f'{new_dir}/{tt}/{class_}')\n",
        "      files = []\n",
        "      for dir in dirs:\n",
        "        files += [f'{dir}/{file}' for file in os.listdir(dir)]\n",
        "      random.shuffle(files)\n",
        "      files = files[:min]\n",
        "      for file in files:\n",
        "        shutil.copy2(file, f'{new_dir}/{tt}/{class_}')\n",
        "        os.remove(file)\n",
        "\n",
        "########\n",
        "class_d = {\n",
        "    'mouth':['_Mask_Chin','no_mask'],\n",
        "    'no_nose':['_Mask_Nose_Mouth','_Mask_Mouth_Chin', 'mask']\n",
        "}\n",
        "########\n",
        "! rm -rf org_data/\n",
        "create_classes(class_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dbd2b2adef5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' rm -rf org_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mcreate_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-dbd2b2adef5d>\u001b[0m in \u001b[0;36mcreate_classes\u001b[0;34m(class_d)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{new_dir}/{tt}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_min_num_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-dbd2b2adef5d>\u001b[0m in \u001b[0;36mget_min_num_files\u001b[0;34m(class_d, tt)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'cv-final-project/data/small/{x}/{tt}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-dbd2b2adef5d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'cv-final-project/data/small/{x}/{tt}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cv-final-project/data/small/_Mask_Chin/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isUjKiqDlgcR"
      },
      "source": [
        "! rm -rf cv-final-project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlukVglwY8Z2"
      },
      "source": [
        "###########HYPERPARAMETERS###########\n",
        "BATCH_SIZE = 64\n",
        "#####################################\n",
        "def get_loader(tt, BATCH_SIZE):\n",
        "  resize = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "  ])\n",
        "  train_data = datasets.ImageFolder(f'org_data/{tt}', transform=resize)\n",
        "  return DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader = get_loader('train', BATCH_SIZE)\n",
        "val_loader = get_loader('test', BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Wit5ZmXQr8"
      },
      "source": [
        "print(len(train_loader.dataset))\n",
        "print(len(os.listdir('org_data/train/mouth')))\n",
        "print(len(os.listdir('org_data/train/no_mouth')))\n",
        "print(len(val_loader.dataset))\n",
        "print(len(os.listdir('org_data/test/mouth')))\n",
        "print(len(os.listdir('org_data/test/no_mouth')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdy-vbHOTS6N"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    ##############################################################################\n",
        "    # TODO: Design your own network, define layers here.                         #\n",
        "    # Here We provide a sample of two-layer fc network from HW4 Part3.           #\n",
        "    # Your solution, however, should contain convolutional layers.               #\n",
        "    # Refer to PyTorch documentations of torch.nn to pick your layers.           #\n",
        "    # (https://pytorch.org/docs/stable/nn.html)                                  #\n",
        "    # Some common choices: Linear, Conv2d, ReLU, MaxPool2d, AvgPool2d, Dropout   #\n",
        "    # If you have many layers, use nn.Sequential() to simplify your code         #\n",
        "    ##############################################################################\n",
        "    self.in_dim = 3\n",
        "    self.mid_layer_params = 32\n",
        "    self.num_classes = 2\n",
        "    self.cnn_layers_max = nn.Sequential(\n",
        "        # Defining a 2D convolution layer\n",
        "        nn.Conv2d(self.in_dim, self.mid_layer_params, kernel_size=3),\n",
        "        nn.BatchNorm2d(self.mid_layer_params),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.cnn_layers_avg = nn.Sequential(\n",
        "        nn.Conv2d(self.mid_layer_params, self.mid_layer_params, kernel_size=3),\n",
        "        nn.BatchNorm2d(self.mid_layer_params),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.cnn_layers_max_2 = nn.Sequential(\n",
        "        nn.Conv2d(self.mid_layer_params, self.mid_layer_params, kernel_size=3),\n",
        "        nn.BatchNorm2d(self.mid_layer_params),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.linear_layers = nn.Sequential(\n",
        "        nn.Linear(self.mid_layer_params*30**2, self.num_classes)\n",
        "    )\n",
        "    \n",
        "    ##############################################################################\n",
        "    #                             END OF YOUR CODE                               #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self,x):\n",
        "    ##############################################################################\n",
        "    # TODO: Design your own network, implement forward pass here                 #\n",
        "    ##############################################################################\n",
        "    x = self.cnn_layers_max(x)\n",
        "    x = self.cnn_layers_avg(x)\n",
        "    x = self.cnn_layers_max_2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear_layers(x)\n",
        "    return x\n",
        "    ##############################################################################\n",
        "    #                             END OF YOUR CODE                               #\n",
        "    ##############################################################################\n",
        "\n",
        "model = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
        "print('Your network:')\n",
        "print(summary(model, (3,256,256))) # visualize your model\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Modify the lines below to experiment with different optimizers,      #\n",
        "# parameters (such as learning rate) and number of epochs.                   #\n",
        "##############################################################################\n",
        "# Set up optimization hyperparameters\n",
        "learning_rate = 1e-2\n",
        "weight_decay = 1e-4\n",
        "num_epoch = 10  # TODO: Choose an appropriate number of training epochs\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                       weight_decay=weight_decay)\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0aIwScpyGve"
      },
      "source": [
        "#External References used\n",
        "## https://discuss.pytorch.org/t/iterating-through-imagefolder-for-sample-target/82291/2\n",
        "## ^how to iterate through ImageFolder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvMCyEI_VfXh"
      },
      "source": [
        "# Code to train the neural net\n",
        "def train(model, trainloader, valloader, num_epoch = 10): # Train the model\n",
        "  print(\"Start training...\")\n",
        "  trn_loss_hist = []\n",
        "  trn_acc_hist = []\n",
        "  val_acc_hist = []\n",
        "  model.train() # Set the model to training mode\n",
        "  for i in range(num_epoch):\n",
        "    running_loss = []\n",
        "    print('-----------------Epoch = %d-----------------' % (i+1))\n",
        "    for batch, label in tqdm(trainloader):\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      optimizer.zero_grad() # Clear gradients from the previous iteration\n",
        "      pred = model(batch) # This will call Network.forward() that you implement\n",
        "      loss = criterion(pred, label) # Calculate the loss\n",
        "      running_loss.append(loss.item())\n",
        "      loss.backward() # Backprop gradients to all tensors in the network\n",
        "      optimizer.step() # Update trainable weights\n",
        "    print(\"\\n Epoch {} loss:{}\".format(i+1,np.mean(running_loss)))\n",
        "\n",
        "    # Keep track of training loss, accuracy, and validation loss\n",
        "    trn_loss_hist.append(np.mean(running_loss))\n",
        "    trn_acc_hist.append(evaluate(model, trainloader))\n",
        "    print(\"\\n Evaluate on validation set...\")\n",
        "    val_acc_hist.append(evaluate(model, valloader))\n",
        "  print(\"Done!\")\n",
        "  return trn_loss_hist, trn_acc_hist, val_acc_hist\n",
        "\n",
        "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
        "  model.eval() # Set the model to evaluation mode\n",
        "  correct = 0\n",
        "  with torch.no_grad(): # Do not calculate grident to speed up computation\n",
        "    for batch, label in tqdm(loader):\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      pred = model(batch)\n",
        "      correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
        "    acc = correct/len(loader.dataset)\n",
        "    print(\"\\n Evaluation accuracy: {}\".format(acc))\n",
        "    return acc\n",
        "    \n",
        "trn_loss_hist, trn_acc_hist, val_acc_hist = train(model, train_loader, \n",
        "                                                  val_loader, num_epoch)\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Note down the evaluation accuracy on test set                        #\n",
        "##############################################################################\n",
        "#print(\"\\n Evaluate on test set\")\n",
        "#evaluate(model, test_loader);\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Submit the accuracy plot                                             #\n",
        "##############################################################################\n",
        "# visualize the training / validation accuracies\n",
        "x = np.arange(num_epoch)\n",
        "# train/val accuracies for MiniVGG\n",
        "plt.figure()\n",
        "plt.plot(x, trn_acc_hist)\n",
        "plt.plot(x, val_acc_hist)\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.xticks(x)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Mouth Classification')\n",
        "plt.gcf().set_size_inches(10, 5)\n",
        "plt.savefig(\"mouth_classifier_performace.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHWhgTO2zR_p"
      },
      "source": [
        "torch.save(model, 'mouth_classifier_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPt65zWc9NOz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}